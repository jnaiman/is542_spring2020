##### BACK TO SLIDES FOR PCA! #####

# reference: https://www.datacamp.com/community/tutorials/pca-analysis-r
library(stats)
data(mtcars)
head(mtcars)

#mpg: Fuel consumption (Miles per (US) gallon): more powerful 
#      and heavier cars tend to consume more fuel.
#cyl: Number of cylinders: more powerful cars often have more cylinders
#disp: Displacement (cu.in.): the combined volume of the engine's cylinders
#hp: Gross horsepower: this is a measure of the power generated by the car
#drat: Rear axle ratio: this describes how a turn 
#     of the drive shaft corresponds to a turn of the wheels. 
#     Higher values will decrease fuel efficiency.
#wt: Weight (1000 lbs): pretty self-explanatory!
#qsec: 1/4 mile time: the cars speed and acceleration
#vs: Engine block: this denotes whether the vehicle's engine 
# is shaped like a "V", or is a more common straight shape.
#am: Transmission: this denotes whether the car's transmission 
#     is automatic (0) or manual (1).
#gear: Number of forward gears: sports cars tend to have more gears.
#carb: Number of carburetors: associated with more powerful engines

# what we'll do now is apply PCA to look for relationships between
#  our variables
#  NOTE: there is no "explanatory" and "response" variables here 
#    we are basically trying to figure out the "shape" of this space
#    how are variables corrleated with eachother?
#   What variable is responsible for the clustering?

# exclude vs and am since they are catagorical
mtcars_min = mtcars[,c(1:7,10,11)]
mtcars.pca = prcomp(mtcars_min, center = TRUE,scale. = TRUE)

# what are our inputs above:
# center:	a logical value indicating whether the variables should be 
#   shifted to be zero centered. Alternately, a vector of length 
#   equal the number of columns of x can be supplied. The value is passed to scale.
# scale:	a logical value indicating whether the variables should 
#   be caled to have unit variance before the analysis takes place. 
#   The default is FALSE for consistency with S, but in general scaling 
#   is advisable. Alternatively, a vector of length equal the number of 
#   columns of x can be supplied. The value is passed to scale.

summary(mtcars.pca)
#Importance of components:
#PC1    PC2     PC3     PC4     PC5     PC6     PC7    PC8     PC9
#Standard deviation     2.3782 1.4429 0.71008 0.51481 0.42797 0.35184 0.32413 0.2419 0.14896
#Proportion of Variance 0.6284 0.2313 0.05602 0.02945 0.02035 0.01375 0.01167 0.0065 0.00247
#Cumulative Proportion  0.6284 0.8598 0.91581 0.94525 0.96560 0.97936 0.99103 0.9975 1.00000

# You obtain 9 principal components, which you call PC1-9. 
# Each of these explains a percentage of the total variation 
# in the dataset. That is to say: PC1 explains 63% of the total 
# variance, which means that nearly two-thirds of the information 
# in the dataset (9 variables) can be encapsulated by just 
# that one Principal Component.

# lets look at parts of the output
print(mtcars.pca$x)
# what is this?  So, for every observation listed, i.e. each car
#  this is the "amount" of the vector in each of the PC components
#  this is essentially each of the data points rotated from 
#  their original axis  i.e. mpg, cyl, disp, hp, etc
#  to these new vectors

# you can sort of think of PC1-PC9 as 9 "axis" and each of these
#  numbers as the coordinate of each car on each of these axis
#  Instead of (x,y) coordinates -> we have a 9-vector coordinate

# *****SHOW ROTATION ANIMATIONS!!!!*****

# we can also make the proportion of varience plot for each PC
# recall that varience = (STD)^2 and we are given the STD for each 
par(mfrow=c(1,1))
PoV = mtcars.pca$sdev^2/sum(mtcars.pca$sdev^2)
plot(PoV, type='l')
# so we can indeed see that the first PC has about 60% of the varience, 
#  while the others have much less

# we might be tempted to reduce the number of variables in 
# our model by ignorning PCs above the ~4th since the proportion
#  of the varience that each of those hold is very small
#  this is somewhat of an art - figuring out when and which PCs to 
#  ignore 
# Stay tuned for your future stats classes!

# while this all has been a bit abstract, its just a quick example of 
# trying to make a model out of a bunch of related & possibly 
# colinear variables
# We could in theory use this model to predict the relationships 
#  between the mpg, weight, etc in a new car model for example

#install.packages("devtools")
library(devtools)
install_github("vqv/ggbiplot")

library(ggbiplot)
ggbiplot(mtcars.pca)
# so, this is projection of all our parameters into the space of PC1 and PC2
#  -> the 2 princicple components that explain *most* of the varience

# what is this saying?
# for example, the variables hp, cyl & disp are very close to the positive PC1 axis
#   this is telling you that if you increase a car's hp, cyl and/or disp it will 
#   Have a much larger "coordinate" on the axis of PC1

# Here "gear" and "carb" are the closes to the PC2 axis -> meaning that if you 
#  have a car with a larger value of "gear" or "carb" it is likely to have a 
#  larger "coordinate" in PC2


# so this is nice, but what car is where?
ggbiplot(mtcars.pca, labels=rownames(mtcars))

# now we are getting somewhere -> we can already tell that a few or our cars that
#  are "similar" are clustered together on this plot: 
# the Maserati Bora, Ferrari Dino and Ford Pantera L all cluster together at the top. 
# This makes sense, as all of these are sports cars.

# whatelse can explain this clustering?  Lets guess that its the country of 
#  origion for these cars:
# here, I'm adding a variable that says what country things are
#  this is just something I knew about each of these cars
mtcars.country = c(rep("Japan", 3), rep("US",4), 
                   rep("Europe", 7),rep("US",3), 
                   "Europe", rep("Japan", 3), rep("US",4), 
                   rep("Europe", 3), "US", rep("Europe", 3))

# choices here specifis plotting pc1 and pc2
ggbiplot(mtcars.pca,ellipse=TRUE, choices=c(1,2),
         labels=rownames(mtcars), groups=mtcars.country)

# Now we see something interesting: the American cars form a distinct cluster to the right. 
# Looking at the axes, you see that the American cars are characterized by high values 
# for cyl, disp, and wt. 
# Japanese cars, on the other hand, are characterized by high mpg. 
# European cars are somewhat in the middle and less tightly clustered than either group.

# IN GROUPS (only if we have time):
#  what about PC3 & PC4?  Any clustering evident?  Why or why not?
#  What about PC8 and PC9?

# ANS:
ggbiplot(mtcars.pca,ellipse=TRUE, choices=c(8,9),
         labels=rownames(mtcars), groups=mtcars.country)
# in both cases -> much less obvious clustering & lots of overlapping circles
#  this is because the higher PC's control much less of the varience
