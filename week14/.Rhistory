sample_data_glm = sample_data
levels(sample_data_glm$y)
sample_data_glm = sample_data
# -1 -> 0
levels(sample_data_glm$y) = c("0", "1")
levels(sample_data_glm$y)
# let's do a logistic regression on this dataset
glm2 = glm(y~x1+x2, data=sample_data_glm, family=binomial)
print(summary(glm2))
# first make a function to predict the probably of "success"
# success = 1 => same thing as "probability of being magenta"
myLogProbFunction = function(x1,x2){
ly2 = glm2$coefficients[1] + glm2$coefficients[2]*x1 +
glm2$coefficients[2]*x2
# extra step
ply2 = exp(ly2)/(1.0+exp(ly2))
return(ply2)
}
print(myLogProbFunction(-1,4))
myLogProbFunction = function(x1,x2){
ly2 = glm2$coefficients[1] + glm2$coefficients[2]*x1 +
glm2$coefficients[3]*x2
# extra step
ply2 = exp(ly2)/(1.0+exp(ly2))
return(ply2)
}
# x1=-1, x2=4 => from the graph we see its probably a blue point
#  so the probablity of "success" i.e. begin a magenta point
#  is very low
print(myLogProbFunction(-1,4))
print(myLogProbFunction(2,2))
# first make a function to predict the probably of "success"
# success = 1 => same thing as "probability of being magenta"
myLogProbFunction = function(x1,x2){
ly2 = glm2$coefficients[1] + glm2$coefficients[2]*x1 +
glm2$coefficients[3]*x2
# extra step
ply2 = exp(ly2)/(1.0+exp(ly2))
return(ply2)
}
# x1=-1, x2=4 => from the graph we see its probably a blue point
#  so the probablity of "success" i.e. begin a magenta point
#  is very low
print(myLogProbFunction(-1,4)) # very small, so unlikey to be pink
print(myLogProbFunction(2,2))
# x1=-1, x2=4 => from the graph we see its probably a blue point
#  so the probablity of "success" i.e. begin a magenta point
#  is very low
print(myLogProbFunction(-1,4)) # very small, so unlikey to be pink
print(myLogProbFunction(2,-2))
print(myLogProbFunction(0,0))
# Plot a decision boundary
# Light "hack" a contour function to plot where the probablity of
#  success, i.e. being pink is 0.5
grid_size = 100 # 100 points in x1 and x2, uniform grid
prob_cut_off = 0.5 # 50% prob of being magenta
myx1_seq = seq(-2, 4, length=grid_size)
myx2_seq = seq(-2, 4, length=grid_size)
# Probablity grid
probGrid = matrix(0, grid_size, grid_size)
probGrid
# Probablity grid
probGrid = matrix(0, grid_size, grid_size)
for (i in 1:grid_size){
for (j in 1:grid_size){
probij = myLogProbFunction(myx1_seq[i], myx2_seq[j])
probij[i,j] = myProb
}
}
# Probablity grid
probGrid = matrix(0, grid_size, grid_size)
for (i in 1:grid_size){
for (j in 1:grid_size){
probij = myLogProbFunction(myx1_seq[i], myx2_seq[j])
probij[i,j] = probij
}
}
grid_size = 100 # 100 points in x1 and x2, uniform grid
prob_cut_off = 0.5 # 50% prob of being magenta
myx1_seq = seq(-2, 4, length=grid_size)
myx2_seq = seq(-2, 4, length=grid_size)
# Probablity grid
probGrid = matrix(0, grid_size, grid_size)
for (i in 1:grid_size){
for (j in 1:grid_size){
probij = myLogProbFunction(myx1_seq[i], myx2_seq[j])
probij[i,j] = probij
}
}
grid_size = 100 # 100 points in x1 and x2, uniform grid
prob_cut_off = 0.5 # 50% prob of being magenta
myx1_seq = seq(-2, 4, length=grid_size)
myx2_seq = seq(-2, 4, length=grid_size)
# Probablity grid
probGrid = matrix(0, grid_size, grid_size)
for (i in 1:grid_size){
for (j in 1:grid_size){
probij = myLogProbFunction(myx1_seq[i], myx2_seq[j])
probGrid[i,j] = probij
}
}
probGrid
# use contour plot to show where probGrid = 0.5
contour(myx1_seq, myx2_seq, probGrid, levels=0.5, labels="")
points(sample_data$x1[sample_data$y==1],
sample_data$x2[sample_data$y==1], col="magenta")
points(sample_data$x1[sample_data$y==-1],
sample_data$x2[sample_data$y==-1], col="blue")
train.X = cbind(sample_data$x1, sample_data$x2)
train.X
grid_size = 10
x1_new = seq(-2,4, length=grid_size) # same as myx1_seq
x2_new = seq(-2,4, length=grid_size) # same as myx2_seq
# formatting again for KNN
test.X = expand.grid(x1_new, x2_new)
test.X
knn_train_prediction = knn(train.X, test.X,
sample_data$y,
k=k, prob=TRUE)
# pick the "k" in our KNN
k = 5
# let's make prediction!
knn_train_prediction = knn(train.X, test.X,
sample_data$y,
k=k, prob=TRUE)
knn_train_prediction
prob = attr(knn_train_prediction,"prob")
prob
prob2 = ifelse(knn_train_prediction=="1", prob, 1-prob)
prob2
prob2 = ifelse(knn_train_prediction=="1", prob, 1-prob)
# for formatting for a contour plot we want this as a matrix
#  across x1 and x2
prob2 = matrix(prob2, grid_size, grid_size)
prob2
# redo the decision boundary plot with our KNN model
contour(x1_new, x2_new, prob2, levels=0.5, labels="")
points(sample_data$x1[sample_data$y==1],
sample_data$x2[sample_data$y==1], col="magenta")
points(sample_data$x1[sample_data$y==-1],
sample_data$x2[sample_data$y==-1], col="blue")
# redo the decision boundary plot with our KNN model
contour(x1_new, x2_new, prob2, levels=0.5, labels="",
xlab='x1', ylab='x2')
points(sample_data$x1[sample_data$y==1],
sample_data$x2[sample_data$y==1], col="magenta")
points(sample_data$x1[sample_data$y==-1],
sample_data$x2[sample_data$y==-1], col="blue")
# start with a small grid of 10 in x1 and 10 in x2
grid_size = 100
x1_new = seq(-2,4, length=grid_size) # same as myx1_seq
x2_new = seq(-2,4, length=grid_size) # same as myx2_seq
# formatting again for KNN
test.X = expand.grid(x1_new, x2_new)
# pick the "k" in our KNN
k = 5
# let's make prediction!
knn_train_prediction = knn(train.X, test.X,
sample_data$y,
k=k, prob=TRUE)
# right now we have prob of belonging to a particular group
prob = attr(knn_train_prediction,"prob")
# what we want to plot a decision boundary is the
#  probably of our grid point being magenta
#  and our decision boundary will be when this prob = 0.5
# This next line does it, don't worry about it too much
prob2 = ifelse(knn_train_prediction=="1", prob, 1-prob)
# for formatting for a contour plot we want this as a matrix
#  across x1 and x2
prob2 = matrix(prob2, grid_size, grid_size)
# redo the decision boundary plot with our KNN model
contour(x1_new, x2_new, prob2, levels=0.5, labels="",
xlab='x1', ylab='x2')
points(sample_data$x1[sample_data$y==1],
sample_data$x2[sample_data$y==1], col="magenta")
points(sample_data$x1[sample_data$y==-1],
sample_data$x2[sample_data$y==-1], col="blue")
# start with a small grid of 10 in x1 and 10 in x2
grid_size = 100
x1_new = seq(-2,4, length=grid_size) # same as myx1_seq
x2_new = seq(-2,4, length=grid_size) # same as myx2_seq
# formatting again for KNN
test.X = expand.grid(x1_new, x2_new)
# pick the "k" in our KNN
k = 50
# let's make prediction!
knn_train_prediction = knn(train.X, test.X,
sample_data$y,
k=k, prob=TRUE)
# right now we have prob of belonging to a particular group
prob = attr(knn_train_prediction,"prob")
# what we want to plot a decision boundary is the
#  probably of our grid point being magenta
#  and our decision boundary will be when this prob = 0.5
# This next line does it, don't worry about it too much
prob2 = ifelse(knn_train_prediction=="1", prob, 1-prob)
# for formatting for a contour plot we want this as a matrix
#  across x1 and x2
prob2 = matrix(prob2, grid_size, grid_size)
# redo the decision boundary plot with our KNN model
contour(x1_new, x2_new, prob2, levels=0.5, labels="",
xlab='x1', ylab='x2')
points(sample_data$x1[sample_data$y==1],
sample_data$x2[sample_data$y==1], col="magenta")
points(sample_data$x1[sample_data$y==-1],
sample_data$x2[sample_data$y==-1], col="blue")
# Let's plot an array of different k's in our KNN models
k_values = c(1, 3, 5, 9, 15, 20, 50, 100, 300)
grid_size = 10
par(mfrow=c(3,3))
for (i in 1:length(k_values)){
# let's make prediction!
knn_train_prediction = knn(train.X, test.X,
sample_data$y,
k=k_values[i], prob=TRUE)
# grab classification prob and transform it for
#  plotting with contour plot
prob = attr(knn_train_prediction,"prob")
# some fancy stuff for formatting
prob2 = ifelse(knn_train_prediction=="1", prob, 1-prob)
prob2 = matrix(prob2, grid_size, grid_size)
# make title labels
titleLab = paste("k = ", toString(k_values[i]))
# redo the decision boundary plot with our KNN model
contour(x1_new, x2_new, prob2, levels=0.5, labels="",
xlab='x1', ylab='x2', main=titleLab)
points(sample_data$x1[sample_data$y==1],
sample_data$x2[sample_data$y==1], col="magenta")
points(sample_data$x1[sample_data$y==-1],
sample_data$x2[sample_data$y==-1], col="blue")
}
# Let's plot an array of different k's in our KNN models
k_values = c(1, 3, 5, 9, 15, 20, 50, 100, 300)
grid_size = 10
par(mfrow=c(3,3))
for (i in 1:length(k_values)){
# let's make prediction!
knn_train_prediction = knn(train.X, test.X,
sample_data$y,
k=k_values[i], prob=TRUE)
# grab classification prob and transform it for
#  plotting with contour plot
prob = attr(knn_train_prediction,"prob")
# some fancy stuff for formatting
prob2 = ifelse(knn_train_prediction=="1", prob, 1-prob)
prob2 = matrix(prob2, grid_size, grid_size)
# make title labels
titleLab = paste("k = ", toString(k_values[i]))
# redo the decision boundary plot with our KNN model
contour(x1_new, x2_new, prob2, levels=0.5, labels="",
xlab='x1', ylab='x2', main=titleLab)
points(sample_data$x1[sample_data$y==1],
sample_data$x2[sample_data$y==1], col="magenta")
points(sample_data$x1[sample_data$y==-1],
sample_data$x2[sample_data$y==-1], col="blue")
}
# Let's plot an array of different k's in our KNN models
k_values = c(1, 3, 5, 9, 15, 20, 50, 100, 300)
grid_size = 10
par(mfrow=c(3,3))
for (i in 1:length(k_values)){
# let's make prediction!
knn_train_prediction = knn(train.X, test.X,
sample_data$y,
k=k_values[i], prob=TRUE)
# grab classification prob and transform it for
#  plotting with contour plot
prob = attr(knn_train_prediction,"prob")
# some fancy stuff for formatting
prob2 = ifelse(knn_train_prediction=="1", prob, 1-prob)
prob2 = matrix(prob2, grid_size, grid_size)
# make title labels
titleLab = paste("k = ", toString(k_values[i]))
# redo the decision boundary plot with our KNN model
contour(x1_new, x2_new, prob2, levels=0.5, labels="",
xlab='x1', ylab='x2', main=titleLab)
points(sample_data$x1[sample_data$y==1],
sample_data$x2[sample_data$y==1], col="magenta")
points(sample_data$x1[sample_data$y==-1],
sample_data$x2[sample_data$y==-1], col="blue")
}
# Let's plot an array of different k's in our KNN models
k_values = c(1, 3, 5, 9, 15, 20, 50, 100, 300)
grid_size = 10
x1_new = seq(-2,4, length=grid_size) # same as myx1_seq
x2_new = seq(-2,4, length=grid_size) # same as myx2_seq
par(mfrow=c(3,3))
for (i in 1:length(k_values)){
# let's make prediction!
knn_train_prediction = knn(train.X, test.X,
sample_data$y,
k=k_values[i], prob=TRUE)
# grab classification prob and transform it for
#  plotting with contour plot
prob = attr(knn_train_prediction,"prob")
# some fancy stuff for formatting
prob2 = ifelse(knn_train_prediction=="1", prob, 1-prob)
prob2 = matrix(prob2, grid_size, grid_size)
# make title labels
titleLab = paste("k = ", toString(k_values[i]))
# redo the decision boundary plot with our KNN model
contour(x1_new, x2_new, prob2, levels=0.5, labels="",
xlab='x1', ylab='x2', main=titleLab)
points(sample_data$x1[sample_data$y==1],
sample_data$x2[sample_data$y==1], col="magenta")
points(sample_data$x1[sample_data$y==-1],
sample_data$x2[sample_data$y==-1], col="blue")
}
# Let's plot an array of different k's in our KNN models
k_values = c(1, 3, 5, 9, 15, 20, 50, 100, 300)
grid_size = 10
x1_new = seq(-2,4, length=grid_size) # same as myx1_seq
x2_new = seq(-2,4, length=grid_size) # same as myx2_seq
par(mfrow=c(3,3))
for (i in 1:length(k_values)){
# let's make prediction!
knn_train_prediction = knn(train.X, test.X,
sample_data$y,
k=k_values[i], prob=TRUE)
# grab classification prob and transform it for
#  plotting with contour plot
prob = attr(knn_train_prediction,"prob")
# some fancy stuff for formatting
prob2 = ifelse(knn_train_prediction=="1", prob, 1-prob)
prob2 = matrix(prob2, grid_size, grid_size)
# make title labels
titleLab = paste("k = ", toString(k_values[i]))
# redo the decision boundary plot with our KNN model
contour(x1_new, x2_new, prob2, levels=0.5, labels="",
xlab='x1', ylab='x2', main=titleLab, lwd=4)
points(sample_data$x1[sample_data$y==1],
sample_data$x2[sample_data$y==1], col="magenta")
points(sample_data$x1[sample_data$y==-1],
sample_data$x2[sample_data$y==-1], col="blue")
}
prob2
# Let's plot an array of different k's in our KNN models
k_values = c(1, 3, 5, 9, 15, 20, 50, 100, 300)
grid_size = 10
x1_new = seq(-2,4, length=grid_size) # same as myx1_seq
x2_new = seq(-2,4, length=grid_size) # same as myx2_seq
test.X = expand.grid(x1_new, x2_new)
par(mfrow=c(3,3))
for (i in 1:length(k_values)){
# let's make prediction!
knn_train_prediction = knn(train.X, test.X,
sample_data$y,
k=k_values[i], prob=TRUE)
# grab classification prob and transform it for
#  plotting with contour plot
prob = attr(knn_train_prediction,"prob")
# some fancy stuff for formatting
prob2 = ifelse(knn_train_prediction=="1", prob, 1-prob)
prob2 = matrix(prob2, grid_size, grid_size)
# make title labels
titleLab = paste("k = ", toString(k_values[i]))
# redo the decision boundary plot with our KNN model
contour(x1_new, x2_new, prob2, levels=0.5, labels="",
xlab='x1', ylab='x2', main=titleLab, lwd=4)
points(sample_data$x1[sample_data$y==1],
sample_data$x2[sample_data$y==1], col="magenta")
points(sample_data$x1[sample_data$y==-1],
sample_data$x2[sample_data$y==-1], col="blue")
}
# ONE more example of KNN - boston cream donut
n_neg = 200
n_pos = 200
sample_data = boston_cream_doughnut(n_neg=n_neg, n_pos=n_pos)
plot(sample_data$x1[sample_data$y==-1],
sample_data$x2[sample_data$y==-1], col='blue')
points(sample_data$x1[sample_data$y==1],
sample_data$x2[sample_data$y==1], col='magenta')
# ONE more example of KNN - boston cream donut
n_neg = 200
n_pos = 200
sample_data = boston_cream_doughnut(n_neg=n_neg, n_pos=n_pos)
plot(sample_data$x1[sample_data$y==-1],
sample_data$x2[sample_data$y==-1], col='blue')
points(sample_data$x1[sample_data$y==1],
sample_data$x2[sample_data$y==1], col='magenta')
# ONE more example of KNN - boston cream donut
n_neg = 200
n_pos = 200
sample_data = boston_cream_doughnut(n_neg=n_neg, n_pos=n_pos)
plot(sample_data$x1[sample_data$y==-1],
sample_data$x2[sample_data$y==-1], col='blue')
points(sample_data$x1[sample_data$y==1],
sample_data$x2[sample_data$y==1], col='magenta')
n_neg = 200
n_pos = 200
sample_data = boston_cream_doughnut(n_neg=n_neg, n_pos=n_pos)
plot(sample_data$x1[sample_data$y==-1],
sample_data$x2[sample_data$y==-1], col='blue',
xlim=c(-4,4), ylim=c(-4,4))
points(sample_data$x1[sample_data$y==1],
sample_data$x2[sample_data$y==1], col='magenta')
# we can use all the things we used before
# Let's plot an array of different k's in our KNN models
k_values = c(1, 3, 5, 9, 15, 20, 50, 100, 300)
grid_size = 10
x1_new = seq(-2,4, length=grid_size) # same as myx1_seq
x2_new = seq(-2,4, length=grid_size) # same as myx2_seq
test.X = expand.grid(x1_new, x2_new)
par(mfrow=c(3,3))
for (i in 1:length(k_values)){
# let's make prediction!
knn_train_prediction = knn(train.X, test.X,
sample_data$y,
k=k_values[i], prob=TRUE)
# grab classification prob and transform it for
#  plotting with contour plot
prob = attr(knn_train_prediction,"prob")
# some fancy stuff for formatting
prob2 = ifelse(knn_train_prediction=="1", prob, 1-prob)
prob2 = matrix(prob2, grid_size, grid_size)
# make title labels
titleLab = paste("k = ", toString(k_values[i]))
# redo the decision boundary plot with our KNN model
contour(x1_new, x2_new, prob2, levels=0.5, labels="",
xlab='x1', ylab='x2', main=titleLab, lwd=4)
points(sample_data$x1[sample_data$y==1],
sample_data$x2[sample_data$y==1], col="magenta")
points(sample_data$x1[sample_data$y==-1],
sample_data$x2[sample_data$y==-1], col="blue")
}
# we can use all the things we used before
# Let's plot an array of different k's in our KNN models
k_values = c(1, 3, 5, 9, 15, 20, 50, 100, 300)
grid_size = 10
x1_new = seq(-2,4, length=grid_size) # same as myx1_seq
x2_new = seq(-2,4, length=grid_size) # same as myx2_seq
test.X = expand.grid(x1_new, x2_new)
par(mfrow=c(3,3))
for (i in 1:length(k_values)){
# let's make prediction!
knn_train_prediction = knn(train.X, test.X,
sample_data$y,
k=k_values[i], prob=TRUE)
# grab classification prob and transform it for
#  plotting with contour plot
prob = attr(knn_train_prediction,"prob")
# some fancy stuff for formatting
prob2 = ifelse(knn_train_prediction=="1", prob, 1-prob)
prob2 = matrix(prob2, grid_size, grid_size)
# make title labels
titleLab = paste("k = ", toString(k_values[i]))
# redo the decision boundary plot with our KNN model
contour(x1_new, x2_new, prob2, levels=0.5, labels="",
xlab='x1', ylab='x2', main=titleLab, lwd=4)
points(sample_data$x1[sample_data$y==1],
sample_data$x2[sample_data$y==1], col="magenta")
points(sample_data$x1[sample_data$y==-1],
sample_data$x2[sample_data$y==-1], col="blue")
}
# Let's plot an array of different k's in our KNN models
k_values = c(1, 3, 5, 9, 15, 20, 50, 100, 300)
grid_size = 10
x1_new = seq(-2,4, length=grid_size) # same as myx1_seq
x2_new = seq(-2,4, length=grid_size) # same as myx2_seq
test.X = expand.grid(x1_new, x2_new)
par(mfrow=c(3,3))
for (i in 1:length(k_values)){
# let's make prediction!
knn_train_prediction = knn(train.X, test.X,
sample_data$y,
k=k_values[i], prob=TRUE)
# grab classification prob and transform it for
#  plotting with contour plot
prob = attr(knn_train_prediction,"prob")
# some fancy stuff for formatting
prob2 = ifelse(knn_train_prediction=="1", prob, 1-prob)
prob2 = matrix(prob2, grid_size, grid_size)
# make title labels
titleLab = paste("k = ", toString(k_values[i]))
# redo the decision boundary plot with our KNN model
contour(x1_new, x2_new, prob2, levels=0.5, labels="",
xlab='x1', ylab='x2', main=titleLab, lwd=4)
points(sample_data$x1[sample_data$y==1],
sample_data$x2[sample_data$y==1], col="magenta")
points(sample_data$x1[sample_data$y==-1],
sample_data$x2[sample_data$y==-1], col="blue")
}
# we can use all the things we used before
# Let's plot an array of different k's in our KNN models
train.X = cbind(sample_data$x1, sample_data$x2)
k_values = c(1, 3, 5, 9, 15, 20, 50, 100, 300)
grid_size = 10
x1_new = seq(-2,4, length=grid_size) # same as myx1_seq
x2_new = seq(-2,4, length=grid_size) # same as myx2_seq
test.X = expand.grid(x1_new, x2_new)
par(mfrow=c(3,3))
for (i in 1:length(k_values)){
# let's make prediction!
knn_train_prediction = knn(train.X, test.X,
sample_data$y,
k=k_values[i], prob=TRUE)
# grab classification prob and transform it for
#  plotting with contour plot
prob = attr(knn_train_prediction,"prob")
# some fancy stuff for formatting
prob2 = ifelse(knn_train_prediction=="1", prob, 1-prob)
prob2 = matrix(prob2, grid_size, grid_size)
# make title labels
titleLab = paste("k = ", toString(k_values[i]))
# redo the decision boundary plot with our KNN model
contour(x1_new, x2_new, prob2, levels=0.5, labels="",
xlab='x1', ylab='x2', main=titleLab, lwd=4)
points(sample_data$x1[sample_data$y==1],
sample_data$x2[sample_data$y==1], col="magenta")
points(sample_data$x1[sample_data$y==-1],
sample_data$x2[sample_data$y==-1], col="blue")
}
